{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a9c9c3-56a4-45ff-a94c-1d10e30d5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data processing, database connection, Telegram API interaction, and logging\n",
    "\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import psycopg2  # PostgreSQL database connection\n",
    "from telethon import TelegramClient  # Telegram API client for data extraction\n",
    "from telethon.sessions import StringSession  # Session management for Telegram client\n",
    "import asyncio  # Asynchronous programming support\n",
    "import nest_asyncio  # Enables nested event loops in Jupyter and other environments\n",
    "import logging  # Logging for debugging and monitoring\n",
    "import re  # Regular expressions for text processing\n",
    "import emoji  # Emoji handling and removal from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a53e86-8b34-4eb2-bce4-dcff7c26138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# Telegram API Credentials (Replace with your values)\n",
    "api_id = \"25516266\"\n",
    "api_hash = \"fa60997cc6b272938bb894a93df300e5\"\n",
    "\n",
    "# PostgreSQL Credentials\n",
    "DB_NAME = \"E2E-DP\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"postgres\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "\n",
    "try:\n",
    "    # Connect to PostgreSQL Database\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Connected to PostgreSQL\")\n",
    "except Exception as e:\n",
    "    print(\"Error connecting to PostgreSQL:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59994389-67b8-4203-adfa-cee5b399e2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001200, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file\n",
    "df_ecom = pd.read_csv(\"Online_Retail.csv\")  \n",
    "print(df_ecom.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5ba0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InvoiceNo StockCode                         Description  Quantity  \\\n",
      "0    536365    85123A  WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1    536365     71053                 WHITE METAL LANTERN         6   \n",
      "2    536365    84406B      CREAM CUPID HEARTS COAT HANGER         8   \n",
      "\n",
      "      InvoiceDate  UnitPrice  CustomerID         Country  \n",
      "0  12/1/2010 8:26       2.55     17850.0  United Kingdom  \n",
      "1  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
      "2  12/1/2010 8:26       2.75     17850.0  United Kingdom  \n"
     ]
    }
   ],
   "source": [
    "#  Display first 3 datas\n",
    "print(df_ecom.head(3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f07247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning completed successfully!\n",
      "Cleaned file saved as cleaned_Online_Rental_data.csv\n"
     ]
    }
   ],
   "source": [
    "# List of columns to clean\n",
    "columns_to_clean = [\"InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"UnitPrice\", \"CustomerID\",\t\"Country\"] \n",
    "\n",
    "# Ensure all columns in the list exist before cleaning\n",
    "missing_columns = [col for col in columns_to_clean if col not in df_ecom.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Error: The following columns are missing: {', '.join(missing_columns)}\")\n",
    "else:\n",
    "    # Drop rows with missing values in the selected columns\n",
    "    df_ecom.dropna(subset=columns_to_clean, inplace=True)\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    df_ecom.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Handle negative values by setting them to 0 (or you can drop rows with negative values)\n",
    "    for col in columns_to_clean:\n",
    "        if col in df_ecom.columns:\n",
    "            df_ecom[col] = df_ecom[col].apply(lambda x: max(0, x) if isinstance(x, (int, float)) else x)\n",
    "\n",
    "    print(\"Cleaning completed successfully!\")\n",
    "\n",
    "     # force convert to int\n",
    "    df_ecom[\"CustomerID\"] = pd.to_numeric(df_ecom[\"CustomerID\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Save the cleaned file\n",
    "df_ecom.to_csv(\"cleaned_Online_Rental_data.csv\", index=False)\n",
    "print(\"Cleaned file saved as cleaned_Online_Rental_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "733d3d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(738757, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned CSV file\n",
    "df_ecom = pd.read_csv(\"cleaned_Online_Rental_data.csv\")  \n",
    "print(df_ecom.shape)  # Display the size of the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308e4e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InvoiceNo StockCode                         Description  Quantity  \\\n",
      "0    536365    85123A  WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1    536365     71053                 WHITE METAL LANTERN         6   \n",
      "2    536365    84406B      CREAM CUPID HEARTS COAT HANGER         8   \n",
      "\n",
      "      InvoiceDate  UnitPrice  CustomerID         Country  \n",
      "0  12/1/2010 8:26       2.55       17850  United Kingdom  \n",
      "1  12/1/2010 8:26       3.39       17850  United Kingdom  \n",
      "2  12/1/2010 8:26       2.75       17850  United Kingdom  \n"
     ]
    }
   ],
   "source": [
    " # Display first 3 datas form the cleared dataset\n",
    "print(df_ecom.head(3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3282f7-74c1-4c46-b89a-6906b32e5e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed in successfully as âœ¨; remember to not break the ToS or you will risk an account ban!\n",
      "Connected to Telegram successfully!\n",
      "Extracted 5000 messages from easybuyethiopia\n",
      "Data saved to easybuyethiopia_messages.csv\n",
      "Extracted 5000 messages from Ecommerceaddis\n",
      "Data saved to Ecommerceaddis_messages.csv\n",
      "Extracted 2288 messages from jiji_shop_ethiopia\n",
      "Data saved to jiji_shop_ethiopia_messages.csv\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()  # Allows running asyncio inside Jupyter\n",
    "\n",
    "# List of Telegram channels to scrape\n",
    "channels = [\n",
    "    \"easybuyethiopia\",\n",
    "    \"Ecommerceaddis\",\n",
    "    \"jiji_shop_ethiopia\"\n",
    "]\n",
    "\n",
    "# Create a single Telegram client session\n",
    "client = TelegramClient(StringSession(), api_id, api_hash)\n",
    "\n",
    "async def scrape_telegram_data(channel_username, limit=5000):\n",
    "    \"\"\"Scrapes messages from Telegram and returns a DataFrame with only the required columns.\"\"\"\n",
    "    try:\n",
    "        messages = []\n",
    "        async for message in client.iter_messages(channel_username, limit=limit):\n",
    "            messages.append({\n",
    "                \"date\": message.date,\n",
    "                \"message\": message.text,\n",
    "                \"user_id\": message.sender_id  # Keeps negative IDs if from groups/channels\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(messages, columns=[\"date\", \"message\", \"user_id\"])  # Only these 3 columns\n",
    "        print(f\"Extracted {len(df)} messages from {channel_username}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data from {channel_username}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"date\", \"message\", \"user_id\"])  # Ensure correct structure\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Runs the scraper for multiple channels and saves each to a separate CSV.\"\"\"\n",
    "    await client.start()\n",
    "    print(\"Connected to Telegram successfully!\")\n",
    "\n",
    "    for channel in channels:\n",
    "        df_telegram = await scrape_telegram_data(channel)\n",
    "        if not df_telegram.empty:\n",
    "            filename = f\"{channel}_messages.csv\"\n",
    "            df_telegram.to_csv(filename, index=False)  # Save with only 3 columns\n",
    "            print(f\"Data saved to {filename}\")\n",
    "        else:\n",
    "            print(f\"No data extracted from {channel}\")\n",
    "\n",
    "    await client.disconnect()\n",
    "\n",
    "# Run the scraper\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35be1116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3) \n",
      "\n",
      "(5000, 3) \n",
      "\n",
      "(2288, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load telegram_message.CSV file\n",
    "df_telegram_1 = pd.read_csv(\"easybuyethiopia_messages.csv\")  \n",
    "print(df_telegram_1.shape,\"\\n\")\n",
    "\n",
    "# Load telegram_message.CSV file\n",
    "df_telegram_2 = pd.read_csv(\"Ecommerceaddis_messages.csv\")  \n",
    "print(df_telegram_2.shape,\"\\n\")\n",
    "\n",
    "# Load telegram_message.CSV file\n",
    "df_telegram_3 = pd.read_csv(\"jiji_shop_ethiopia_messages.csv\")  \n",
    "print(df_telegram_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73ab5855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max user_id: -1001226361445 Min user_id: -1001226361445\n",
      "Max user_id: -1001337500007 Min user_id: -1001337500007\n",
      "Max user_id: -1001400160497 Min user_id: -1001400160497\n",
      "Max CustomerID: 18287 Min CustomerID: 12346\n"
     ]
    }
   ],
   "source": [
    "print(\"Max user_id:\", df_telegram_1[\"user_id\"].max(), \"Min user_id:\", df_telegram_1[\"user_id\"].min())\n",
    "print(\"Max user_id:\", df_telegram_2[\"user_id\"].max(), \"Min user_id:\", df_telegram_2[\"user_id\"].min())\n",
    "print(\"Max user_id:\", df_telegram_3[\"user_id\"].max(), \"Min user_id:\", df_telegram_3[\"user_id\"].min())\n",
    "print(\"Max CustomerID:\", df_ecom[\"CustomerID\"].max(), \"Min CustomerID:\", df_ecom[\"CustomerID\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48efc4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        date  \\\n",
      "0  2024-11-17 10:06:26+00:00   \n",
      "1  2024-11-17 10:06:26+00:00   \n",
      "2  2024-11-17 10:06:26+00:00   \n",
      "\n",
      "                                             message        user_id  \n",
      "0  *The Most Sold G-Shock Model Is Now Available*... -1001226361445  \n",
      "1                                                NaN -1001226361445  \n",
      "2                                                NaN -1001226361445  \n",
      "                        date  \\\n",
      "0  2025-02-07 15:51:30+00:00   \n",
      "1  2025-02-07 09:10:48+00:00   \n",
      "2  2025-02-07 05:18:01+00:00   \n",
      "\n",
      "                                             message        user_id  \n",
      "0  ğŸ’¥Mi 360Â° rotation security camera 2K\\n\\nğŸ‘‰áˆµáˆ« á‰¦á‰³... -1001337500007  \n",
      "1  ğŸ“£ Multifunctional Shoe and Hat Rack\\n\\nâœ”ï¸ á‰£áˆˆ á‹˜... -1001337500007  \n",
      "2  âœ”ï¸ á‹‰á‹µ á‹°áŠ•á‰ áŠá‰»á‰½áŠ• áŠ¨á‰³á‰½ á‹«áˆµá‰€áˆ˜áŒ¥áŠá‹áŠ• áˆŠáŠ•áŠ­ á‰°áŒ«áŠá‹ á‹¨áˆáŒ†á‰½ á‹•á‰ƒ á‹¨áˆš... -1001337500007  \n",
      "                        date  \\\n",
      "0  2025-02-07 07:13:22+00:00   \n",
      "1  2025-02-07 07:13:21+00:00   \n",
      "2  2025-02-07 07:13:21+00:00   \n",
      "\n",
      "                                             message        user_id  \n",
      "0  Samsung galaxy A 05 \\nGb 64\\nRam 4\\nPrice ğŸ’°11.... -1001400160497  \n",
      "1                                                NaN -1001400160497  \n",
      "2                                                NaN -1001400160497  \n"
     ]
    }
   ],
   "source": [
    "#  Display first 3 datas\n",
    "print(df_telegram_1.head(3)) \n",
    "print(df_telegram_2.head(3)) \n",
    "print(df_telegram_3.head(3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b702b5bb-42d8-4175-80f9-0d8610428605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 18:56:30,306 - INFO - Loaded easybuyethiopia_messages.csv: Shape before cleaning -> (5000, 3)\n",
      "2025-02-07 18:56:30,317 - INFO - ğŸ—‘ Removed 2286 duplicate rows.\n",
      "2025-02-07 18:56:31,013 - INFO - Cleaned data saved to cleaned_easybuyethiopia_messages.csv: Final shape -> (2714, 3)\n",
      "\n",
      "2025-02-07 18:56:31,198 - INFO - Loaded Ecommerceaddis_messages.csv: Shape before cleaning -> (5000, 3)\n",
      "2025-02-07 18:56:31,219 - INFO - ğŸ—‘ Removed 837 duplicate rows.\n",
      "2025-02-07 18:56:33,793 - INFO - Cleaned data saved to cleaned_Ecommerceaddis_messages.csv: Final shape -> (4163, 3)\n",
      "\n",
      "2025-02-07 18:56:33,793 - INFO - Loaded jiji_shop_ethiopia_messages.csv: Shape before cleaning -> (2288, 3)\n",
      "2025-02-07 18:56:33,793 - INFO - ğŸ—‘ Removed 1056 duplicate rows.\n",
      "2025-02-07 18:56:33,945 - INFO - Cleaned data saved to cleaned_jiji_shop_ethiopia_messages.csv: Final shape -> (1232, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# List of Telegram CSV files\n",
    "csv_files = [\n",
    "    \"easybuyethiopia_messages.csv\",\n",
    "    \"Ecommerceaddis_messages.csv\",\n",
    "    \"jiji_shop_ethiopia_messages.csv\"\n",
    "]\n",
    "\n",
    "# List of columns to clean\n",
    "columns_to_clean = [\"user_id\"]\n",
    "text_columns = [\"message\"]\n",
    "date_column = \"date\"\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Remove emojis from a given text.\"\"\"\n",
    "    return emoji.replace_emoji(text, replace=\"\") if isinstance(text, str) else text\n",
    "\n",
    "# Loop through each file, clean it, and save\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        # Load the CSV file efficiently\n",
    "        df_telegram = pd.read_csv(file, encoding=\"utf-8\")\n",
    "        logging.info(f\"Loaded {file}: Shape before cleaning -> {df_telegram.shape}\")\n",
    "\n",
    "        # Check if required columns exist\n",
    "        missing_columns = [col for col in columns_to_clean + text_columns + [date_column] if col not in df_telegram.columns]\n",
    "        if missing_columns:\n",
    "            logging.error(f\"âš  The following columns are missing in {file}: {', '.join(missing_columns)}\")\n",
    "            continue  # Skip to next file\n",
    "\n",
    "        # Remove duplicate rows\n",
    "        initial_rows = df_telegram.shape[0]\n",
    "        df_telegram.drop_duplicates(inplace=True)\n",
    "        logging.info(f\"ğŸ—‘ Removed {initial_rows - df_telegram.shape[0]} duplicate rows.\")\n",
    "\n",
    "        # Ensure correct data type for user_id\n",
    "        df_telegram[\"user_id\"] = pd.to_numeric(df_telegram[\"user_id\"], errors=\"coerce\").fillna(0).astype(\"Int64\")\n",
    "\n",
    "        # Ensure no user_id values exceed PostgreSQL BIGINT range\n",
    "        df_telegram = df_telegram[\n",
    "            (df_telegram[\"user_id\"] >= -9223372036854775808) & \n",
    "            (df_telegram[\"user_id\"] <= 9223372036854775807)\n",
    "        ]\n",
    "\n",
    "        # Convert \"date\" column to proper datetime format\n",
    "        df_telegram[date_column] = pd.to_datetime(df_telegram[date_column], errors=\"coerce\")\n",
    "\n",
    "        # Remove rows with invalid dates\n",
    "        df_telegram.dropna(subset=[date_column], inplace=True)\n",
    "\n",
    "        # Fill missing text values with \"unknown\"\n",
    "        for col in text_columns:\n",
    "            df_telegram[col] = df_telegram[col].fillna(\"unknown\")\n",
    "\n",
    "        # Remove emojis from text columns\n",
    "        for col in text_columns:\n",
    "            df_telegram[col] = df_telegram[col].apply(remove_emojis)\n",
    "\n",
    "        # Save cleaned data\n",
    "        cleaned_filename = f\"cleaned_{file}\"\n",
    "        df_telegram.to_csv(cleaned_filename, index=False, encoding=\"utf-8\")\n",
    "        logging.info(f\"Cleaned data saved to {cleaned_filename}: Final shape -> {df_telegram.shape}\\n\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"{file} not found. Ensure the scraper ran successfully.\\n\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while processing {file}: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5de553a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2714, 3) \n",
      "\n",
      "(4163, 3) \n",
      "\n",
      "(1232, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned cleaned_telegram_messagesCSV file\n",
    "df_telegram_1 = pd.read_csv(\"cleaned_easybuyethiopia_messages.csv\")  \n",
    "print(df_telegram_1.shape,\"\\n\")  # Display the size of the cleaned dataset\n",
    "\n",
    "df_telegram_2 = pd.read_csv(\"cleaned_Ecommerceaddis_messages.csv\")  \n",
    "print(df_telegram_2.shape,\"\\n\")  \n",
    "\n",
    "df_telegram_3 = pd.read_csv(\"cleaned_jiji_shop_ethiopia_messages.csv\")  \n",
    "print(df_telegram_3.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f99dc0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max user_id: -1001226361445 Min user_id: -1001226361445\n",
      "Max user_id: -1001337500007 Min user_id: -1001337500007\n",
      "Max user_id: -1001400160497 Min user_id: -1001400160497\n",
      "Max CustomerID: 18287 Min CustomerID: 12346\n"
     ]
    }
   ],
   "source": [
    "print(\"Max user_id:\", df_telegram_1[\"user_id\"].max(), \"Min user_id:\", df_telegram_1[\"user_id\"].min())\n",
    "print(\"Max user_id:\", df_telegram_2[\"user_id\"].max(), \"Min user_id:\", df_telegram_2[\"user_id\"].min())\n",
    "print(\"Max user_id:\", df_telegram_3[\"user_id\"].max(), \"Min user_id:\", df_telegram_3[\"user_id\"].min())\n",
    "print(\"Max CustomerID:\", df_ecom[\"CustomerID\"].max(), \"Min CustomerID:\", df_ecom[\"CustomerID\"].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0c1b2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InvoiceNo       object\n",
      "StockCode       object\n",
      "Description     object\n",
      "Quantity         int64\n",
      "InvoiceDate     object\n",
      "UnitPrice      float64\n",
      "CustomerID       int64\n",
      "Country         object\n",
      "dtype: object \n",
      "\n",
      "date       object\n",
      "message    object\n",
      "user_id     int64\n",
      "dtype: object \n",
      "\n",
      "date       object\n",
      "message    object\n",
      "user_id     int64\n",
      "dtype: object \n",
      "\n",
      "date       object\n",
      "message    object\n",
      "user_id     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print the data type of the col\n",
    "print(df_ecom.dtypes,\"\\n\")\n",
    "print(df_telegram_1.dtypes,\"\\n\")\n",
    "print(df_telegram_2.dtypes,\"\\n\")\n",
    "print(df_telegram_3.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd39bb0e-033d-471b-9269-5b4ce500dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, host=DB_HOST, port=DB_PORT\n",
    ")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d72668c-1981-4bf8-80eb-3b7d1424c651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables created successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Drop the table if it exists and recreate it because i face problems with this database table\n",
    "    cursor.execute(\"\"\"\n",
    "        DROP TABLE IF EXISTS Rental_Dataset;\n",
    "    \"\"\")\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE Rental_Dataset (\n",
    "            order_id SERIAL PRIMARY KEY,\n",
    "            InvoiceNo VARCHAR(50),\n",
    "            StockCode VARCHAR(20),\n",
    "            Description TEXT,\n",
    "            Quantity INTEGER,\n",
    "            InvoiceDate TIMESTAMP,\n",
    "            UnitPrice DECIMAL(10, 2),\n",
    "            CustomerID BIGINT,\n",
    "            Country VARCHAR(100)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS telegram_messages (\n",
    "            message_id SERIAL PRIMARY KEY,\n",
    "            date TIMESTAMP,\n",
    "            message TEXT,\n",
    "            user_id TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    print(\"Tables created successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    conn.rollback()  # Rollback the transaction if there is an error\n",
    "    print(\"Error creating tables:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8029030a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Retail Data: (738757, 8)\n",
      "Successfully inserted 738757 records into Rental_Dataset.\n",
      "Processing cleaned_easybuyethiopia_messages.csv: Shape -> (2714, 3)\n",
      "Successfully inserted 2714 records from cleaned_easybuyethiopia_messages.csv into PostgreSQL.\n",
      "Processing cleaned_Ecommerceaddis_messages.csv: Shape -> (4163, 3)\n",
      "Successfully inserted 4163 records from cleaned_Ecommerceaddis_messages.csv into PostgreSQL.\n",
      "Processing cleaned_jiji_shop_ethiopia_messages.csv: Shape -> (1232, 3)\n",
      "Successfully inserted 1232 records from cleaned_jiji_shop_ethiopia_messages.csv into PostgreSQL.\n",
      "Data insertion completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# ---- Insert Retail Data into PostgreSQL ----\n",
    "try:\n",
    "    df_ecom = pd.read_csv(\"cleaned_Online_Rental_data.csv\")\n",
    "    print(f\"Loaded Retail Data: {df_ecom.shape}\")\n",
    "\n",
    "    Retail_data = list(df_ecom.itertuples(index=False, name=None))\n",
    "\n",
    "    cursor.executemany(\"\"\"\n",
    "        INSERT INTO Rental_Dataset (InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", Retail_data)\n",
    "\n",
    "    conn.commit()\n",
    "    print(f\"Successfully inserted {len(Retail_data)} records into Rental_Dataset.\")\n",
    "\n",
    "except Exception as e:\n",
    "    conn.rollback()\n",
    "    print(f\"Error inserting Retail data: {e}\")\n",
    "\n",
    "# ---- Insert Telegram Data into PostgreSQL ----\n",
    "csv_files = [\n",
    "    \"cleaned_easybuyethiopia_messages.csv\",\n",
    "    \"cleaned_Ecommerceaddis_messages.csv\",\n",
    "    \"cleaned_jiji_shop_ethiopia_messages.csv\"\n",
    "]\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df_telegram = pd.read_csv(file)\n",
    "        print(f\"Processing {file}: Shape -> {df_telegram.shape}\")\n",
    "\n",
    "        # Ensure correct data types\n",
    "        df_telegram[\"user_id\"] = df_telegram[\"user_id\"].astype(str)  # Store as string\n",
    "        df_telegram[\"date\"] = pd.to_datetime(df_telegram[\"date\"], errors=\"coerce\")\n",
    "\n",
    "        # Drop rows with missing required fields\n",
    "        df_telegram.dropna(subset=[\"date\", \"message\", \"user_id\"], inplace=True)\n",
    "\n",
    "        # Convert DataFrame to list of tuples for bulk insertion\n",
    "        telegram_data = list(df_telegram[[\"date\", \"message\", \"user_id\"]].itertuples(index=False, name=None))\n",
    "\n",
    "        if telegram_data:  # Insert only if there's data\n",
    "            cursor.executemany(\"\"\"\n",
    "                INSERT INTO telegram_messages (date, message, user_id)\n",
    "                VALUES (%s, %s, %s)   \n",
    "            \"\"\", telegram_data)\n",
    "            conn.commit()\n",
    "            print(f\"Successfully inserted {len(telegram_data)} records from {file} into PostgreSQL.\")\n",
    "        else:\n",
    "            print(f\"No valid data to insert from {file}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"Data insertion completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b3db4b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Retail Data (Sample):\n",
      "   order_id invoiceno stockcode                          description  \\\n",
      "0         1    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER   \n",
      "1         2    536365     71053                  WHITE METAL LANTERN   \n",
      "2         3    536365    84406B       CREAM CUPID HEARTS COAT HANGER   \n",
      "3         4    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE   \n",
      "4         5    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.   \n",
      "5         6    536365     22752         SET 7 BABUSHKA NESTING BOXES   \n",
      "6         7    536365     21730    GLASS STAR FROSTED T-LIGHT HOLDER   \n",
      "7         8    536366     22633               HAND WARMER UNION JACK   \n",
      "8         9    536366     22632            HAND WARMER RED POLKA DOT   \n",
      "9        10    536367     84879        ASSORTED COLOUR BIRD ORNAMENT   \n",
      "\n",
      "   quantity         invoicedate unitprice  customerid         country  \n",
      "0         6 2010-12-01 08:26:00      2.55       17850  United Kingdom  \n",
      "1         6 2010-12-01 08:26:00      3.39       17850  United Kingdom  \n",
      "2         8 2010-12-01 08:26:00      2.75       17850  United Kingdom  \n",
      "3         6 2010-12-01 08:26:00      3.39       17850  United Kingdom  \n",
      "4         6 2010-12-01 08:26:00      3.39       17850  United Kingdom  \n",
      "5         2 2010-12-01 08:26:00      7.65       17850  United Kingdom  \n",
      "6         6 2010-12-01 08:26:00      4.25       17850  United Kingdom  \n",
      "7         6 2010-12-01 08:28:00      1.85       17850  United Kingdom  \n",
      "8         6 2010-12-01 08:28:00      1.85       17850  United Kingdom  \n",
      "9        32 2010-12-01 08:34:00      1.69       13047  United Kingdom  \n",
      "\n",
      " elegram Messages (Sample):\n",
      "   message_id  user_id                                            message  \\\n",
      "0       99610        0  áŠ áˆµá‰¸áŠ³á‹­!!!!Â  áˆˆáˆ½á‹«áŒ­ á‹¨á‰€áˆ¨á‰  áˆ˜áŠªáŠ“\\nğŸ‘‡ğŸ‘‡ğŸ‘‡\\nopel coras \\nMo...   \n",
      "1       99611        0                                            unknown   \n",
      "2       99612        0  ğŸ áŠ áˆµá‰¸áŠ³á‹­ áˆˆáˆ½á‹«áŒ­ á‹¨á‰€áˆ¨á‰ Â  á‰¤á‰µ ğŸ  \\nğŸ‘‰ á‰¦á‰³á¡ áˆƒá‹‹áˆ³ á–áˆŠá‰´áŠ­áŠ’áŠ­ áŠ áˆŠá‰¶/...   \n",
      "3       99613        0  ğŸ  áˆˆáˆ½á‹«áŒ­ á‹¨á‰€áˆ¨á‰  áŠ®áŠá‹¶áˆšáŠ•á‹¨áˆğŸ  \\nğŸ‘‰á‰¦á‰³á¡áˆƒá‹‹áˆ³ á‹ˆ/áŠ áˆ›áŠ‘áŠ¤áˆÂ  áŠ®áŠ•á‹¶áˆšáŠ•á‹¨...   \n",
      "4       99614        0  ğŸ”¥á‹¨á‰´áˆŒáŒáˆ«áˆ á‰»áŠ“áˆ‹á‰½áŠ• áˆ‹á‹­ á‰ á…áˆá á‹¨áˆáŠ•áˆˆá‰ƒá‰¸á‹áŠ• áˆ˜áˆ¨áŒƒá‹á‰½ á‰ á‰²áŠ­á‰¶áŠ­ á”áŒƒá‰½...   \n",
      "5       99615        0  ğŸ áˆˆáˆ½á‹«áŒ­ á‹¨á‰€áˆ¨á‰  á‹˜áˆ˜áŠ“á‹Š áˆºáˆ‹ á‰¤á‰µ ğŸ  \\nğŸ‘‰ á‰¦á‰³á¡ áˆƒá‹‹áˆ³ á‹¶áˆ® áŠ¥áˆ­á‰£á‰³ áŠ áŠ«...   \n",
      "6       99616        0  #1ğŸ áŠ áˆµá‰¸áŠ³á‹­ áˆˆáˆ½á‹«áŒ­á‹¨á‰€áˆ¨á‰  á‰¤á‰µğŸ \\nÂ Â Â Â Â Â Â Â Â Â Â Â  á‹­ááŒ áŠ‘ \\ná‰¦á‰³á¡...   \n",
      "7       99617        0  ğŸ 01 áŠ áŠ«á‰£á‰¢ áˆˆáˆ½á‹«áŒ­ á‹¨á‰€áˆ¨á‰¡ áˆˆáŒáŠ•á‰£á‰³ á‹¨áˆšáˆ†áŠ• á‰£á‹¶ á‰¦á‰³ ğŸ  \\n#1 ğŸ‘‰á‰¦á‰³...   \n",
      "8       99618        0                                            unknown   \n",
      "9       99619        0                                            unknown   \n",
      "\n",
      "                 date  \n",
      "0 2025-02-01 10:29:47  \n",
      "1 2025-02-01 10:29:47  \n",
      "2 2025-02-01 10:22:17  \n",
      "3 2025-01-31 17:25:30  \n",
      "4 2025-01-30 07:53:43  \n",
      "5 2025-01-30 07:45:57  \n",
      "6 2025-01-29 16:27:10  \n",
      "7 2025-01-29 10:42:59  \n",
      "8 2025-01-28 11:57:08  \n",
      "9 2025-01-28 11:57:07  \n",
      "âœ… PostgreSQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch data from PostgreSQL\n",
    "def fetch_data(query):\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        data = cursor.fetchall()\n",
    "        col_names = [desc[0] for desc in cursor.description]  # Get column names\n",
    "        return pd.DataFrame(data, columns=col_names)\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return None\n",
    "\n",
    "# Retrieve The First 10 Data from Rental_Dataset  database\n",
    "query_retail = \"SELECT * FROM Rental_Dataset LIMIT 10;\"  # Modify as needed\n",
    "df_retail = fetch_data(query_retail)\n",
    "print(\"\\n Retail Data (Sample):\")\n",
    "print(df_retail)\n",
    "\n",
    "# Retrieve The First 10 Messages from telegram_messages database\n",
    "query_telegram = \"SELECT * FROM telegram_messages LIMIT 10;\"  # Modify as needed\n",
    "df_telegram = fetch_data(query_telegram)\n",
    "print(\"\\n telegram Messages (Sample):\")\n",
    "print(df_telegram)\n",
    "\n",
    "\n",
    "# Close the connection\n",
    "if cursor:\n",
    "    cursor.close()\n",
    "if conn:\n",
    "    conn.close()\n",
    "print(\"âœ… PostgreSQL connection closed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
